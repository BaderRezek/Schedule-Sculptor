# ğŸ§© RAG â€” Course Catalog Retrieval Pipeline

This repository powers the **Schedule Sculptor RAG system**, a retrieval-augmented pipeline that turns unstructured university catalog data into structured, searchable, and chunked course information ready for large-language-model applications.

---

## ğŸ“š Overview

The goal of this pipeline is to **extract, clean, and prepare academic catalog data** for intelligent querying â€” for example, â€œWhich UIC classes require Calculus II?â€ or â€œFind electives with no prerequisites in Computer Science.â€

It does this by scraping the public UIC catalog, structuring the course metadata, and exporting clean text chunks optimized for vector-based retrieval.

---

## ğŸ§  Pipeline Flow

data/raw/catalog.py
â†“
raw DataFrame of courses
â†“
processed/rag_export/
â”œâ”€â”€ rag_docs.jsonl   â† full course documents
â”œâ”€â”€ rag_chunks.jsonl â† chunked text for embeddings
â”œâ”€â”€ rag_docs.csv     â† tabular for inspection
â””â”€â”€ rag_chunks.csv   â† chunk-level table
â†“
src/ingest.py â†’ future embedding & retrieval stage

---

## ğŸ“‚ Folder Structure
rag/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ catalog.ipynb   â† interactive scraper
â”‚   â”‚   â”œâ”€â”€ catalog.py      â† CLI scraper version
â”‚   â”‚   â””â”€â”€ init.py
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ rag_export/     â† final outputs (JSONL & CSV)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py           â† ingestion script (planned)
â”‚   â””â”€â”€ init.py
â”‚
â””â”€â”€ README.md               â† you are here

---

## âš™ï¸ How It Works

1. **Scraper (`data/raw/catalog.py`)**
   - Collects all UIC subject URLs.
   - Parses titles, credits, and descriptions.
   - Detects and separates prerequisites, corequisites, and background info.
   - Handles exceptions (e.g., â€œCredit or concurrent registration in ...â€).

2. **Processing**
   - Normalizes messy text into structured columns.
   - Builds unique, stable course IDs and deduplicates overlapping entries.

3. **RAG Export**
   - Creates two layers of outputs:
     - **Docs**: one per course, with full metadata.
     - **Chunks**: overlapping ~500-word text slices for embedding.
   - Saves both JSONL and CSV for flexibility.

4. **Ingestion (coming soon)**
   - Will embed chunks using OpenAI, Hugging Face, or sentence-transformers.
   - Enables semantic search, retrieval-augmented QA, and schedule planning tools.

---

## ğŸ“¦ Outputs

| File | Description |
|------|--------------|
| `rag_docs.jsonl` | One JSON object per course with metadata |
| `rag_chunks.jsonl` | Smaller overlapping text chunks for embeddings |
| `rag_docs.csv` | Flattened course-level data |
| `rag_chunks.csv` | Flattened chunk-level data |

All outputs are stored in: data/processed/rag_export/

---

## ğŸ§° Requirements

```bash
conda install -c conda-forge pandas beautifulsoup4 requests pyarrow
# or
pip install pandas beautifulsoup4 requests pyarrow
```
---

## ğŸš€ Run the Scraper

From the data/raw/ folder:
python catalog.py

This will generate and export the processed files automatically.


